{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeches\n",
    "\n",
    "Adapted from: Judd, Nicholas, Dan Drinkard, Jeremy Carbaugh, and Lindsay Young. *congressional-record: A parser for the Congressional Record.* Chicago, IL: 2017, https://github.com/unitedstates/congressional-record.\n",
    "\n",
    "Please see LICENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "root_dir = '/home/ubuntu/Notebooks/Data_speeches'\n",
    "filename_list = []\n",
    "for filename in glob.glob(root_dir + '/2016/**/json/*.json', recursive=True):\n",
    "    filename_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get speeches from files\n",
    "import os\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "speeches, speeches_json = [], []\n",
    "for filename in filename_list:\n",
    "    with open(filename) as f:\n",
    "        speech = json.load(f)\n",
    "        f.close()\n",
    "        content = json_normalize(speech, 'content', meta = ['id','doc_title', 'title'])\n",
    "        header = json_normalize(speech['header'])\n",
    "        header['id'] = content['id']\n",
    "        speech_normalized = content.merge(header, on = 'id')\n",
    "        speeches.append(speech_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "speeches_df = pd.concat(speeches, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speeches of all document titles:  136148\n",
      "Number of speeches of all document titles:  110269\n"
     ]
    }
   ],
   "source": [
    "# Select speeches meant to persuade (instead of procedural speeches)\n",
    "print(\"Number of speeches of all document titles: \", speeches_df.shape[0])\n",
    "speeches_to_drop =  (['TEXT OF AMENDMENTS', 'NOMINATIONS', 'ADDITIONAL COSPONSORS', 'CONFIRMATIONS', \n",
    "                      'PERSONAL EXPLANATION', 'SENATE COMMITTEE MEETINGS', \n",
    "                      'INTRODUCTION OF BILLS AND JOINT RESOLUTIONS', 'AUTHORITY FOR COMMITTEES TO MEET', 'PLEDGE OF ALLEGIANCE'])\n",
    "substrings_to_drop = (['RECOGNIZING', 'REMEMBERING', 'THANKING', 'Constitutional Authority Statement', \n",
    "                       'ADJOURNMENT', 'TRIBUTE', 'HONORING', 'CONGRATULATING'])\n",
    "for substring in substrings_to_drop:\n",
    "    speeches_df = speeches_df[~speeches_df['doc_title'].str.contains(substring)]\n",
    "\n",
    "speeches_df2 = speeches_df[~speeches_df['doc_title'].isin(speeches_to_drop)]\n",
    "print(\"Number of speeches of all document titles: \", speeches_df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speeches of all kinds:  110269\n",
      "speech            49661\n",
      "title             26012\n",
      "linebreak         16751\n",
      "recorder          11589\n",
      "Unknown            4798\n",
      "clerk               823\n",
      "metacharacters      632\n",
      "empty_line            3\n",
      "Name: kind, dtype: int64\n",
      "Number of speeches (speech):  75673\n"
     ]
    }
   ],
   "source": [
    "# Select speeches meant to persuade (instead of procedural speeches)\n",
    "print(\"Number of speeches of all kinds: \", speeches_df2.shape[0])\n",
    "print(speeches_df2['kind'].value_counts())\n",
    "#list(speeches_df[(speeches_df['kind'] == 'Unknown')]['text'])\n",
    "speeches_df3 = speeches_df2[(speeches_df2['kind'] == 'speech') | (speeches_df2['kind'] == 'title')]\n",
    "print(\"Number of speeches (speech): \", speeches_df3.shape[0])\n",
    "speeches_df3.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>chamber</th>\n",
       "      <th>day</th>\n",
       "      <th>doc_title</th>\n",
       "      <th>extension</th>\n",
       "      <th>id</th>\n",
       "      <th>itemno</th>\n",
       "      <th>kind</th>\n",
       "      <th>month</th>\n",
       "      <th>num</th>\n",
       "      <th>pages</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_bioguide</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>turn</th>\n",
       "      <th>vol</th>\n",
       "      <th>wkday</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5135</td>\n",
       "      <td>Senate</td>\n",
       "      <td>14</td>\n",
       "      <td>EXECUTIVE SESSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CREC-2016-07-14-pt1-PgS5181-4</td>\n",
       "      <td>4</td>\n",
       "      <td>speech</td>\n",
       "      <td>July</td>\n",
       "      <td>114</td>\n",
       "      <td>S5181-S5182</td>\n",
       "      <td>Ms. MURKOWSKI</td>\n",
       "      <td>M001153</td>\n",
       "      <td>Ms. MURKOWSKI. Mr. President, I ask unanimou...</td>\n",
       "      <td>EXECUTIVE SESSION</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5136</td>\n",
       "      <td>Senate</td>\n",
       "      <td>14</td>\n",
       "      <td>EXECUTIVE SESSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CREC-2016-07-14-pt1-PgS5181-4</td>\n",
       "      <td>5</td>\n",
       "      <td>speech</td>\n",
       "      <td>July</td>\n",
       "      <td>114</td>\n",
       "      <td>S5181-S5182</td>\n",
       "      <td>The PRESIDING OFFICER</td>\n",
       "      <td>None</td>\n",
       "      <td>The PRESIDING OFFICER. Without objection, it...</td>\n",
       "      <td>EXECUTIVE SESSION</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5138</td>\n",
       "      <td>Senate</td>\n",
       "      <td>14</td>\n",
       "      <td>EXECUTIVE SESSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CREC-2016-07-14-pt1-PgS5181-4</td>\n",
       "      <td>7</td>\n",
       "      <td>speech</td>\n",
       "      <td>July</td>\n",
       "      <td>114</td>\n",
       "      <td>S5181-S5182</td>\n",
       "      <td>Ms. MURKOWSKI</td>\n",
       "      <td>M001153</td>\n",
       "      <td>Ms. MURKOWSKI. Mr. President, I ask for a di...</td>\n",
       "      <td>EXECUTIVE SESSION</td>\n",
       "      <td>2</td>\n",
       "      <td>162</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5139</td>\n",
       "      <td>Senate</td>\n",
       "      <td>14</td>\n",
       "      <td>EXECUTIVE SESSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CREC-2016-07-14-pt1-PgS5181-4</td>\n",
       "      <td>8</td>\n",
       "      <td>speech</td>\n",
       "      <td>July</td>\n",
       "      <td>114</td>\n",
       "      <td>S5181-S5182</td>\n",
       "      <td>The PRESIDING OFFICER</td>\n",
       "      <td>None</td>\n",
       "      <td>The PRESIDING OFFICER. A division vote has b...</td>\n",
       "      <td>EXECUTIVE SESSION</td>\n",
       "      <td>3</td>\n",
       "      <td>162</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5354</td>\n",
       "      <td>Senate</td>\n",
       "      <td>14</td>\n",
       "      <td>LEGISLATIVE SESSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CREC-2016-07-14-pt1-PgS5182-2</td>\n",
       "      <td>0</td>\n",
       "      <td>speech</td>\n",
       "      <td>July</td>\n",
       "      <td>114</td>\n",
       "      <td>S5182</td>\n",
       "      <td>The PRESIDING OFFICER</td>\n",
       "      <td>None</td>\n",
       "      <td>The PRESIDING OFFICER. The Senate will now r...</td>\n",
       "      <td>LEGISLATIVE SESSION</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index chamber day            doc_title  extension  \\\n",
       "0   5135  Senate  14    EXECUTIVE SESSION      False   \n",
       "1   5136  Senate  14    EXECUTIVE SESSION      False   \n",
       "2   5138  Senate  14    EXECUTIVE SESSION      False   \n",
       "3   5139  Senate  14    EXECUTIVE SESSION      False   \n",
       "4   5354  Senate  14  LEGISLATIVE SESSION      False   \n",
       "\n",
       "                              id  itemno    kind month  num        pages  \\\n",
       "0  CREC-2016-07-14-pt1-PgS5181-4       4  speech  July  114  S5181-S5182   \n",
       "1  CREC-2016-07-14-pt1-PgS5181-4       5  speech  July  114  S5181-S5182   \n",
       "2  CREC-2016-07-14-pt1-PgS5181-4       7  speech  July  114  S5181-S5182   \n",
       "3  CREC-2016-07-14-pt1-PgS5181-4       8  speech  July  114  S5181-S5182   \n",
       "4  CREC-2016-07-14-pt1-PgS5182-2       0  speech  July  114        S5182   \n",
       "\n",
       "                 speaker speaker_bioguide  \\\n",
       "0          Ms. MURKOWSKI          M001153   \n",
       "1  The PRESIDING OFFICER             None   \n",
       "2          Ms. MURKOWSKI          M001153   \n",
       "3  The PRESIDING OFFICER             None   \n",
       "4  The PRESIDING OFFICER             None   \n",
       "\n",
       "                                                text                title  \\\n",
       "0    Ms. MURKOWSKI. Mr. President, I ask unanimou...    EXECUTIVE SESSION   \n",
       "1    The PRESIDING OFFICER. Without objection, it...    EXECUTIVE SESSION   \n",
       "2    Ms. MURKOWSKI. Mr. President, I ask for a di...    EXECUTIVE SESSION   \n",
       "3    The PRESIDING OFFICER. A division vote has b...    EXECUTIVE SESSION   \n",
       "4    The PRESIDING OFFICER. The Senate will now r...  LEGISLATIVE SESSION   \n",
       "\n",
       "   turn  vol     wkday  year  \n",
       "0     0  162  Thursday  2016  \n",
       "1     1  162  Thursday  2016  \n",
       "2     2  162  Thursday  2016  \n",
       "3     3  162  Thursday  2016  \n",
       "4     0  162  Thursday  2016  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Cleaning the speech text\n",
    "speeches_df3['text'] = [text.replace('\\n', '').replace('  ', '') for text in speeches_df3['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "speeches_df3['month'] = [datetime.datetime.strptime(month, '%B').strftime('%m') for month in speeches_df3['month']]\n",
    "speeches_df3['date'] = pd.to_datetime(speeches_df3[['year','month','day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Extracting time / date features\\n# month / year\\nspeeches_df4 = speeches_df4.assign(year_month=pd.to_datetime(speeches_df4[['year', 'month']].assign(day=1)))\\n# how long ago?\\ndate_now = pd.to_datetime('2018-07-27')\\nspeeches_df4['days_ago'] = [(date_now - date).days for date in speeches_df4['date']]\\n\\nspeeches_df4 = speeches_df4[['id','speaker_bioguide', 'text', 'date', 'wkday', 'year_month', 'days_ago']] \\n\""
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "speeches_df4 = speeches_df3.loc[:,~speeches_df3.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df4 = speeches_df3[['chamber', 'doc_title', 'extension', 'id', 'itemno',\n",
    "        'num', 'pages', 'speaker', 'speaker_bioguide', 'text',\n",
    "       'title', 'turn', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "speeches_df4.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 13)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('speeches_cleaned_2016.pkl', 'wb')\n",
    "pickle.dump(speeches_df4, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('speeches_cleaned.pkl', 'wb')\n",
    "pickle.dump(speeches_df4, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge in metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already created metadata for 2017-8\n",
    "# Create list of files of metadata to parse\n",
    "import glob\n",
    "root_dir = '/home/ubuntu/Notebooks/Data_speeches'\n",
    "metadata_list = []\n",
    "for metadata in glob.glob(root_dir + '/2014/**/mods.xml', recursive=True):\n",
    "    metadata_list.append(metadata)\n",
    "for metadata in glob.glob(root_dir + '/2015/**/mods.xml', recursive=True):\n",
    "    metadata_list.append(metadata)\n",
    "for metadata in glob.glob(root_dir + '/2016/**/mods.xml', recursive=True):\n",
    "    metadata_list.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode a list inside a Dataframe cell into separate rows\n",
    "def stack_unstack(df, col_target):\n",
    "    return (df[col_target].apply(pd.Series)\n",
    "            .stack()\n",
    "            .reset_index(level=1, drop=True)\n",
    "            .to_frame(col_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  0.0 %\n",
      "Progress:  9.487666034155598 %\n",
      "Progress:  18.975332068311197 %\n",
      "Progress:  28.46299810246679 %\n",
      "Progress:  37.95066413662239 %\n",
      "Progress:  47.43833017077799 %\n",
      "Progress:  56.92599620493358 %\n",
      "Progress:  66.41366223908919 %\n",
      "Progress:  75.90132827324479 %\n",
      "Progress:  85.38899430740038 %\n",
      "Progress:  94.87666034155598 %\n"
     ]
    }
   ],
   "source": [
    "# Convert xml into a pandas dataframe\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "for idx, meta_data in enumerate(metadata_list):\n",
    "    speeches_bill_ids_all = pd.DataFrame()\n",
    "    if idx % 50 == 0:\n",
    "        print(\"Progress: \", idx / len(metadata_list) * 100, \"%\")\n",
    "    with open(meta_data, 'r') as f:\n",
    "        mods_data = f.read()\n",
    "    f.close()\n",
    "    soup = BeautifulSoup(mods_data, 'lxml')\n",
    "    list_dicts = []\n",
    "    bill_accessid_dict = defaultdict(list)\n",
    "    for i in soup.find_all('bill'):\n",
    "        meta_data_dict = {}\n",
    "        for data in ['congress', 'number', 'type']:\n",
    "            meta_data_dict[data] = i[data]\n",
    "        #print(i)\n",
    "        bill = [b.parent for b in soup.find_all('bill', congress = i['congress'], context = i['context'], number = i['number'], type = i['type'])]\n",
    "        #print(bill)\n",
    "        try:\n",
    "            accessid = [b.find('accessid') for b in bill]\n",
    "            accessid_list = [a.text for a in accessid]\n",
    "            meta_data_dict['accessids'] = accessid_list\n",
    "        except:\n",
    "            continue\n",
    "        list_dicts.append(meta_data_dict)\n",
    "        meta_data_df = pd.DataFrame(list_dicts)\n",
    "        accessids_unstacked = stack_unstack(meta_data_df, 'accessids')\n",
    "        speeches_bill_ids = meta_data_df[['congress', 'number', 'type']].merge(accessids_unstacked, left_index = True, right_index = True)\n",
    "        if speeches_bill_ids_all.empty is True:\n",
    "            speeches_bill_ids_all = speeches_bill_ids\n",
    "        else:\n",
    "            speeches_bill_ids_all = pd.concat([speeches_bill_ids_all,speeches_bill_ids], axis = 0, join = 'outer', ignore_index = True)\n",
    "    try:\n",
    "        speeches_bill_ids_all.drop_duplicates(subset = ['congress', 'number', 'type'], inplace = True)\n",
    "    except(KeyError):\n",
    "        continue\n",
    "    speeches_bill_ids_all.to_csv('metadata/metadata_{idx}'.format(idx = idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('speeches_cleaned.pkl', 'rb')\n",
    "speeches_cleaned = pickle.load(pkl_file)\n",
    "\n",
    "unique_ids = list(speeches_cleaned['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409395, 13)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the parsed metadata\n",
    "import glob\n",
    "import pandas as pd\n",
    "root_dir = '/home/ubuntu/Notebooks/metadata'\n",
    "metadata_df_all = pd.DataFrame()\n",
    "for metadata in glob.glob(root_dir + '/*'):\n",
    "    metadata_df = pd.read_csv(metadata)\n",
    "    # Drop the metadata if it doesn't correspond to an id for the selected speeches\n",
    "    metadata_df = metadata_df[metadata_df['accessids'].isin(unique_ids)]\n",
    "    if metadata_df_all.empty is True:\n",
    "        metadata_df_all = metadata_df\n",
    "    else:\n",
    "        metadata_df_all = pd.concat([metadata_df_all, metadata_df], axis = 0, ignore_index = True)\n",
    "        metadata_df_all = metadata_df_all.drop_duplicates(subset = ['accessids', 'congress', 'number', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format bill ids to match the bill ids taken in by the API\n",
    "metadata_df_all['type'] = [t.lower() for t in metadata_df_all['type']]\n",
    "metadata_df_all['bill_id'] = metadata_df_all['type'] + metadata_df_all['number'].map(int).map(str)\n",
    "\n",
    "metadata_df_all = metadata_df_all[['accessids', 'bill_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_cleaned['index'] = speeches_cleaned.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge speeches and metadata\n",
    "speeches_metadata = speeches_cleaned.merge(metadata_df_all, left_on = 'id', right_on = 'accessids', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speeches_metadata.groupby('bill_id')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('speeches_metadata.pkl', 'wb')\n",
    "pickle.dump(speeches_metadata, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from the speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentions think tank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: http://www.citizensource.com/Opinion&Policy/ThinkTanks.htm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "think_tanks_response = requests.get('http://www.citizensource.com/Opinion&Policy/ThinkTanks.htm')\n",
    "think_tanks_text = think_tanks_response.text\n",
    "think_tanks_soup = BeautifulSoup(think_tanks_text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "for table in think_tanks_soup.find_all('a'): \n",
    "    think_tank = \" \".join(table.text.split())\n",
    "    #print(think_tank)\n",
    "    tables.append(think_tank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def isplit(iterable,splitters):\n",
    "    return [list(g) for k,g in itertools.groupby(iterable,lambda x:x in splitters) if not k]\n",
    "\n",
    "think_tank_list = isplit(tables,'p')\n",
    "\n",
    "think_tank_orientation = think_tank_list[0]\n",
    "think_tank_dict = {}\n",
    "for idx, think_tank in enumerate(think_tank_list[1:]):\n",
    "    think_tank_dict[think_tank_orientation[idx]] = think_tank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "republican = ['RIGHT', 'LIBERTARIAN', 'CONSERVATIVE']\n",
    "democrat = ['LIBERAL', 'LEFT']\n",
    "think_tank_party_dict = defaultdict(list)\n",
    "for key, value in think_tank_dict.items(): \n",
    "    if key in republican:\n",
    "        think_tank_party_dict['Republican'].append(value)\n",
    "    elif key in democrat:\n",
    "        think_tank_party_dict['Democrat'].append(value)\n",
    "    else:\n",
    "        think_tank_party_dict['Non-partisan'].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in think_tank_party_dict.items():\n",
    "    think_tank_party_dict[key] = [item for sublist in value for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('think_tanks.pkl', 'wb')\n",
    "pickle.dump(think_tank_party_dict, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('think_tanks.pkl', 'rb')\n",
    "think_tank_party_dict = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('speeches_metadata.pkl', 'rb')\n",
    "speeches_cleaned = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(976770, 16)\n",
      "131203\n"
     ]
    }
   ],
   "source": [
    "print(speeches_cleaned.shape)\n",
    "print(len(speeches_cleaned['text'].unique()))\n",
    "unique_speeches = speeches_cleaned['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_cleaned.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy matching of mentioning think tank name\n",
    "from fuzzywuzzy import process\n",
    "speeches_cleaned_tt = (pd.DataFrame(columns = ['text','mentions_think_tank_Republican', \n",
    "                        'mentions_think_tank_Democrat', 'mentions_think_tank_Non-partisan',\n",
    "                        'mentions_think_tank_Republican_no', 'mentions_think_tank_Democrat_no',\n",
    "                          'mentions_think_tank_Non-partisan_no']))\n",
    "\n",
    "for idx, sp in enumerate(unique_speeches):\n",
    "    for key, value in think_tank_party_dict.items():\n",
    "        speeches_cleaned_tt.at[idx,'text'] = sp\n",
    "        fuzzy_name = [process.extractOne(sp, v) for v in value]\n",
    "        fuzzies = [fuzzies[0] if fuzzies[1] >=95 else '' for fuzzies in fuzzy_name]\n",
    "        if all(v for v in fuzzies) is True:\n",
    "            print(sp)\n",
    "            print(fuzzies)\n",
    "            speeches_cleaned_tt.at[idx,'mentions_think_tank_{party}'.format(party = key)] = fuzzies\n",
    "            speeches_cleaned_tt.at[idx,'mentions_think_tank_{party}_no'.format(party = key)] = len(fuzzies)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_cleaned = speeches_cleaned.merge(speeches_cleaned_tt, on = 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentions of non-partisan agencies which provide research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "non_partisan_agency = re.compile('''CRS|C\\s+R\\s+S|C.R.S|C.\\s+R.\\s+S.|Congressional Research Service|\n",
    "                                CBO|C\\s+B\\s+O|C.B.O|C.\\s+B.\\s+O.|Congressional Budget Office|\n",
    "                                GAO|G\\s+A\\s+O|G.A.O|G.\\s+A.\\s+O.|Government Accountability Office''')\n",
    "speeches_cleaned['mentions_non_partisan_agency'] = ''\n",
    "speeches_cleaned['mentions_non_partisan_agency_no'] = 0\n",
    "for idx, sp in enumerate(speeches_cleaned['text']):\n",
    "    agencies_mentioned = re.findall(non_partisan_agency,speeches_cleaned.loc[idx,'text'])\n",
    "    speeches_cleaned.at[idx,'mentions_non_partisan_agency'] = agencies_mentioned\n",
    "    speeches_cleaned.at[idx,'mentions_non_partisan_agency_no'] = len(agencies_mentioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentions statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Only keep relevant numbers which relate to statistics\n",
    "numbers_keep = re.compile('''[0-9]+(?=\\s+percent)|one-quarter|one-third|one-half|two-quarters|two-thirds''')\n",
    "# Find all numbers to check I'm not missing any relevant patterns\n",
    "numbers_all = re.compile('[a-zA-Z]+[\\s\\!\\\"\\#\\$\\%\\&\\\\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~]+[0-9]+[\\s\\!\\\"\\#\\$\\%\\&\\\\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~]+[a-zA-Z]+')\n",
    "speeches_cleaned['uses_statistics'] = 0\n",
    "for idx, sp in enumerate(speeches_cleaned['text']):\n",
    "    speeches_cleaned.at[idx,'uses_statistics'] = len(re.findall(numbers_keep,speeches_cleaned.loc[idx,'text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'chamber', 'doc_title', 'extension', 'id', 'itemno', 'num',\n",
       "       'pages', 'speaker', 'speaker_bioguide', 'text', 'title', 'turn', 'date',\n",
       "       'index', 'accessids', 'bill_id', 'mentions_think_tank_Republican',\n",
       "       'mentions_think_tank_Democrat', 'mentions_think_tank_Non-partisan',\n",
       "       'mentions_think_tank_Republican_no', 'mentions_think_tank_Democrat_no',\n",
       "       'mentions_think_tank_Non-partisan_no', 'mentions_non_partisan_agency',\n",
       "       'mentions_non_partisan_agency_no', 'uses_statistics'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106255"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(speeches_cleaned['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping text - no need for the text \n",
    "speeches_cleaned = (speeches_cleaned[['mentions_think_tank_Republican_no', 'mentions_think_tank_Democrat_no',\n",
    "                                    'mentions_think_tank_Non-partisan_no', 'mentions_non_partisan_agency_no',\n",
    "                                    'uses_statistics', 'id','speaker_bioguide', 'date', 'index']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('speeches_features.pkl', 'wb')\n",
    "pickle.dump(speeches_cleaned, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging speeches and metadata\n",
    "\n",
    "- Calculate summary stats for speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "pkl_file = open('speeches_features.pkl', 'rb')\n",
    "speeches_features = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('speeches_similarity.pkl', 'rb')\n",
    "speeches_similarity = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_similarity_df = pd.DataFrame(speeches_similarity)\n",
    "speeches_similarity_df['index'] = speeches_similarity_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_metadata = speeches_features.merge(speeches_similarity_df, on = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['level_0', 'chamber', 'doc_title', 'extension', 'id', 'itemno', 'num',\n",
       "       'pages', 'speaker', 'speaker_bioguide', 'text', 'title', 'turn', 'date',\n",
       "       'index', 'accessids', 'bill_id', 'mentions_think_tank_Republican',\n",
       "       'mentions_think_tank_Democrat', 'mentions_think_tank_Non-partisan',\n",
       "       'mentions_think_tank_Republican_no', 'mentions_think_tank_Democrat_no',\n",
       "       'mentions_think_tank_Non-partisan_no', 'mentions_non_partisan_agency',\n",
       "       'mentions_non_partisan_agency_no', 'uses_statistics', 'crs_sim_avg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congress\n",
    "import datetime\n",
    "start_end_113 = (datetime.date(2013, 1, 3), datetime.date(2015, 1, 3))\n",
    "start_end_114 = (datetime.date(2015, 1, 6), datetime.date(2017, 1, 3))\n",
    "start_end_115 = (datetime.date(2017, 1, 3), datetime.date(2019, 1, 3))\n",
    "start_end_list = [start_end_113, start_end_114, start_end_115]\n",
    "congress_no = list(range(113,116))    \n",
    "start_end_dict = dict(zip(congress_no, start_end_list))\n",
    "speeches_metadata['date'] = [x.date()if type(x) is not datetime.date else x for x in speeches_metadata['date'] ]\n",
    "for congress, start_end in start_end_dict.items():\n",
    "    mask = (speeches_metadata['date'] > start_end[0]) & (speeches_metadata['date'] <= start_end[1])\n",
    "    speeches_metadata.loc[mask,'congress'] = congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_metadata.loc[speeches_metadata['congress'].isnull().sum(), 'congress'] = 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include these summary statistics\n",
    "\n",
    "evidence_keys = ['mentions_non_partisan_agency_no', 'uses_statistics', 'crs_sim_avg']\n",
    "\n",
    "evidence_stats = ['mean', 'median', 'max', 'std']\n",
    "\n",
    "evidence_dict = {}\n",
    "for key in evidence_keys:\n",
    "    evidence_dict[key] = evidence_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:543: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Calculate descriptive stats to summarise the speeches for each bill\n",
    "# Create dataset whose unique id is the bill\n",
    "bill_evidence_stats = speeches_metadata.groupby('bill_id').agg(evidence_dict)\n",
    "speeches_metadata_evidence = speeches_metadata[['accessids', 'bill_id', 'speaker_bioguide', 'congress']].merge(bill_evidence_stats, on = 'bill_id', how = 'inner')\n",
    "cols_new = []\n",
    "for col in speeches_metadata_evidence.columns[4:]:\n",
    "    col_new = col[0] + '_' + col[1]\n",
    "    cols_new.append(col_new)\n",
    "speeches_metadata_evidence.rename(columns = dict(zip(speeches_metadata_evidence.columns[4:], cols_new)), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['accessids', 'bill_id', 'speaker_bioguide', 'congress',\n",
       "       'mentions_non_partisan_agency_no_mean',\n",
       "       'mentions_non_partisan_agency_no_median',\n",
       "       'mentions_non_partisan_agency_no_max',\n",
       "       'mentions_non_partisan_agency_no_std', 'uses_statistics_mean',\n",
       "       'uses_statistics_median', 'uses_statistics_max', 'uses_statistics_std',\n",
       "       'crs_sim_avg_mean', 'crs_sim_avg_median', 'crs_sim_avg_max',\n",
       "       'crs_sim_avg_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_metadata_evidence.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_dummies = pd.get_dummies(speeches_metadata_evidence['speaker_bioguide'], prefix = 'speaker_id')\n",
    "speeches_metadata_evidence = pd.concat([speeches_metadata_evidence,speaker_dummies], axis = 1)\n",
    "speeches_metadata_evidence2 = speeches_metadata_evidence.drop_duplicates(subset = ['bill_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('speeches_metadata_evidence.pkl', 'wb')\n",
    "pickle.dump(speeches_metadata_evidence2, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect bill information if mentioned in a speech\n",
    "\n",
    "ProPublica Congress API, https://projects.propublica.org/api-docs/congress-api/, last access: 07/30/2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_file = open('speeches_metadata_evidence.pkl', 'rb')\n",
    "speeches_metadata = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_metadata.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3898, 693)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "714\n",
      "715\n",
      "863\n",
      "2376\n",
      "3615\n",
      "3616\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy\n",
    "headers = {'X-API-Key': '######'}\n",
    "responses_bills = []\n",
    "for idx in range(0,speeches_metadata.shape[0]):\n",
    "    try:\n",
    "        bill = 'https://api.propublica.org/congress/v1/{congress}/bills/{bill_id}.json'.format(congress = int(speeches_metadata.loc[idx,'congress']), bill_id = speeches_metadata.loc[idx,'bill_id'])\n",
    "        response = requests.get(bill, headers = headers)\n",
    "        #print(response.json())\n",
    "        responses_bills.append(response.json())\n",
    "    except:\n",
    "        print(idx)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "465\n"
     ]
    }
   ],
   "source": [
    "# Count number of bills with votes\n",
    "house_votes, senate_votes = [], []\n",
    "for res in responses_bills:\n",
    "    try:\n",
    "        house_vote = res['results'][0]['house_passage_vote']\n",
    "        if house_vote is not None:\n",
    "            house_votes.append(house_vote)\n",
    "        senate_vote = res['results'][0]['senate_passage_vote']\n",
    "        if senate_vote is not None:\n",
    "            senate_votes.append(senate_vote)\n",
    "    except:\n",
    "        continue\n",
    "print(len(house_votes))\n",
    "print(len(senate_votes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the bills mentioned\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "mentioned_bills_all = pd.DataFrame()\n",
    "for idx, bill in enumerate(responses_bills):\n",
    "    try:\n",
    "        mentioned_bills = json_normalize(bill, 'results')\n",
    "        mentioned_bills_votes = json_normalize(bill, ['results', 'votes'])\n",
    "        mentioned_bills_votes = mentioned_bills_votes.add_prefix('votes_')\n",
    "        mentioned_bills_votes['bill_id'] = mentioned_bills['bill_id'][0]\n",
    "        mentioned_bills_both = mentioned_bills.merge(mentioned_bills_votes, on = 'bill_id', how = 'outer')\n",
    "        if mentioned_bills_all.empty is True:\n",
    "            mentioned_bills_all = mentioned_bills_both\n",
    "        else:\n",
    "            mentioned_bills_all = pd.concat([mentioned_bills_all,mentioned_bills_both], axis = 0, join = 'outer', ignore_index = True)\n",
    "    except(KeyError):\n",
    "        print(\"Key error at: \", idx)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2360"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of bills with votes\n",
    "mentioned_bills_all['votes_roll_call'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('mentioned_bills.pkl', 'wb')\n",
    "pickle.dump(mentioned_bills_all, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5541, 51)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentioned_bills_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_bills_all['last_vote'] = pd.to_datetime(mentioned_bills_all['last_vote'], format = '%Y-%m-%d')\n",
    "mentioned_bills_all['votes_date'] = pd.to_datetime(mentioned_bills_all['votes_date'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable to indicate that one than one party sponsored the bill\n",
    "for idx, row in enumerate(mentioned_bills_all):\n",
    "    if len(mentioned_bills_all.loc[idx, 'cosponsors_by_party'].keys()) > 1:\n",
    "        mentioned_bills_all.loc[idx, 'cosponsored_by_mt1_party'] = 1\n",
    "    else:\n",
    "        mentioned_bills_all.loc[idx, 'cosponsored_by_mt1_party'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable to indicate that the bill was sponsored reasonably equally (40-50% by the second largest sponsoring party)\n",
    "import heapq\n",
    "for idx, row in enumerate(mentioned_bills_all):\n",
    "    co_sponsorship = mentioned_bills_all.loc[idx, 'cosponsors_by_party'].values()\n",
    "    ratio = [v / max(co_sponsorship) for v in co_sponsorship]\n",
    "    try:\n",
    "        if heapq.nlargest(2, ratio)[1] > 0.4:\n",
    "            mentioned_bills_all.loc[idx, 'cosponsored_ratio_mr40pc'] = 1\n",
    "        else:\n",
    "            mentioned_bills_all.loc[idx, 'cosponsored_ratio_mr40pc'] = 0\n",
    "    except:\n",
    "        mentioned_bills_all.loc[idx, 'cosponsored_ratio_mr40pc'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['actions', 'active', 'bill', 'bill_id', 'bill_slug', 'bill_type',\n",
       "       'bill_uri', 'committee_codes', 'committees', 'congress',\n",
       "       'congressdotgov_url', 'cosponsors', 'cosponsors_by_party', 'enacted',\n",
       "       'govtrack_url', 'gpo_pdf_uri', 'house_passage', 'house_passage_vote',\n",
       "       'introduced_date', 'last_vote', 'latest_major_action',\n",
       "       'latest_major_action_date', 'number', 'primary_subject',\n",
       "       'senate_passage', 'senate_passage_vote', 'short_title', 'sponsor',\n",
       "       'sponsor_id', 'sponsor_party', 'sponsor_state', 'sponsor_title',\n",
       "       'sponsor_uri', 'subcommittee_codes', 'summary', 'summary_short',\n",
       "       'title', 'versions', 'vetoed', 'votes', 'votes_api_url',\n",
       "       'votes_chamber', 'votes_date', 'votes_question', 'votes_result',\n",
       "       'votes_roll_call', 'votes_time', 'votes_total_no',\n",
       "       'votes_total_not_voting', 'votes_total_yes', 'withdrawn_cosponsors',\n",
       "       'cosponsored_by_mt1_party', 'cosponsored_ratio_mr40pc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentioned_bills_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject nicheness is a normalised frequency count of the number of times it's mentioned in Congress\n",
    "# ?? Risk of differences due to small counts for nicher subjects?\n",
    "mentioned_bills_all['subject_nicheness'] = mentioned_bills_all.groupby('primary_subject')['primary_subject'].transform('count') / mentioned_bills_all['primary_subject'].value_counts()[0]\n",
    "\n",
    "# Number of months the primary subject is mentioned\n",
    "# month / year\n",
    "mentioned_bills_all['month'] = [x.month for x in mentioned_bills_all['votes_date']]\n",
    "mentioned_bills_all['year'] = [x.year for x in mentioned_bills_all['votes_date']]\n",
    "mentioned_bills_all = mentioned_bills_all.assign(year_month=pd.to_datetime(mentioned_bills_all[['year', 'month']].assign(day=1)))\n",
    "df_primary_subject_trends = mentioned_bills_all.groupby(['primary_subject','year_month'])['bill_id'].count().reset_index()\n",
    "df_primary_subject_trends.rename(columns = {'id': 'primary_subject_number_of_mentions'}, inplace = True)\n",
    "df_primary_subject_trends_month = df_primary_subject_trends.groupby('primary_subject')['year_month'].count().reset_index()\n",
    "df_primary_subject_trends_month.rename(columns = {'year_month': 'primary_suject_no_months_mentioned'}, inplace = True)\n",
    "df_primary_subject_trends = df_primary_subject_trends.merge(df_primary_subject_trends_month, on = 'primary_subject')\n",
    "mentioned_bills_all = mentioned_bills_all.merge(df_primary_subject_trends, on = ['primary_subject', 'year_month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_bills_all = mentioned_bills_all[['active', 'bill_slug', 'committees', 'congress',\n",
    "            'enacted', 'primary_subject', 'sponsor_id','sponsor_party', 'sponsor_state', \n",
    "            'vetoed', 'votes_api_url','votes_chamber', 'votes_date','votes_time', \n",
    "            'votes_total_no', 'votes_total_not_voting','votes_total_yes', 'cosponsored_by_mt1_party',\n",
    "           'cosponsored_ratio_mr40pc', 'primary_suject_no_months_mentioned', 'subject_nicheness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113     647\n",
       "114    1000\n",
       "115     713\n",
       "Name: congress, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentioned_bills_all['congress'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('mentioned_bills.pkl', 'wb')\n",
    "pickle.dump(mentioned_bills_all, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get voting information of bills mentioned\n",
    "\n",
    "ProPublica Congress API, https://projects.propublica.org/api-docs/congress-api/, last access: 07/30/2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_file = open('mentioned_bills.pkl', 'rb')\n",
    "mentioned_bills_all = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_bills_all['votes_api_url_2'] = mentioned_bills_all['votes_api_url'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_votes = mentioned_bills_all['votes_api_url_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2360"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_url is the api call for the votes associated with the bill\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "headers = {'X-API-Key': '######'}\n",
    "\n",
    "responses = {}\n",
    "for vote_url in unique_votes:\n",
    "    if vote_url != '':\n",
    "        response = requests.get(vote_url, headers = headers)\n",
    "        responses[vote_url] = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Get votes broken down by party\n",
    "multiple_party_votes_all = pd.DataFrame()\n",
    "for idx, res in responses.items():\n",
    "    voting_positions = pd.DataFrame(responses[idx]['results']['votes']['vote']['positions'])\n",
    "    voting_positions['bill_id'] = responses[idx]['results']['votes']['vote']['bill']['bill_id']\n",
    "    voting_positions['vote_api'] = idx\n",
    "    voting_positions['chamber'] = responses[idx]['results']['votes']['vote']['chamber']\n",
    "    voting_positions['date'] = responses[idx]['results']['votes']['vote']['date']\n",
    "    voting_positions['time'] = responses[idx]['results']['votes']['vote']['time']\n",
    "    multiple_party_votes = pd.DataFrame()\n",
    "    for party in ['democratic', 'republican', 'independent']:\n",
    "        party_votes = pd.DataFrame([responses[idx]['results']['votes']['vote'][party]])\n",
    "        party_votes = party_votes.add_prefix('{party}_'.format(party = party))\n",
    "        if multiple_party_votes.empty is True:\n",
    "            multiple_party_votes = party_votes\n",
    "        else:\n",
    "            multiple_party_votes = pd.concat([multiple_party_votes, party_votes], axis = 1, join = 'outer')\n",
    "    multiple_party_votes['vote_api'] = idx\n",
    "    party_votes2 = multiple_party_votes.merge(voting_positions, on = 'vote_api', how = 'right')\n",
    "    if multiple_party_votes_all.empty is True:\n",
    "        multiple_party_votes_all = party_votes2\n",
    "    else:\n",
    "        multiple_party_votes_all = pd.concat([multiple_party_votes_all, party_votes2], axis = 0, join = 'outer')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes           437362\n",
       "No            315032\n",
       "Not Voting     26456\n",
       "Speaker         1594\n",
       "Present          283\n",
       "Name: vote_position, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_party_votes_all['vote_position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((multiple_party_votes_all['vote_position'] == 'Yes') | \n",
    "        (multiple_party_votes_all['vote_position'] == 'No') |\n",
    "        (multiple_party_votes_all['vote_position'] == 'Not Voting'))\n",
    "multiple_party_votes_all = multiple_party_votes_all[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "multiple_party_votes_all['date'] = pd.to_datetime(multiple_party_votes_all['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "date_now = pd.to_datetime('2018-07-27')\n",
    "multiple_party_votes_all['days_ago'] = [(date_now - date).days for date in multiple_party_votes_all['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_party_votes_all.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vote with or against the party / abstain\n",
    "multiple_party_votes_all.loc[multiple_party_votes_all['vote_position'] == 'Not Voting','vote_position'] = 'No'\n",
    "party_abbr_dict = {'republican': 'R', 'democratic': 'D'}\n",
    "multiple_party_votes_all['vote_with_party'] = 0\n",
    "for key, value in party_abbr_dict.items():\n",
    "    for idx in range(0, multiple_party_votes_all.shape[0]):\n",
    "        #print(multiple_party_votes_all.loc[idx, '{party}_majority_position'.format(party = key)])\n",
    "        #print(multiple_party_votes_all.loc[idx,'vote_position'])\n",
    "        #print(multiple_party_votes_all.loc[idx,'party'] == '{party_abbr}'.format(party_abbr = value))\n",
    "        if ((multiple_party_votes_all.loc[idx, '{party}_majority_position'.format(party = key)] == multiple_party_votes_all.loc[idx,'vote_position'])\n",
    "        and (multiple_party_votes_all.loc[idx,'party'] == '{party_abbr}'.format(party_abbr = value))):\n",
    "            multiple_party_votes_all.loc[idx, 'vote_with_party'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    708537\n",
       "0     70313\n",
       "Name: vote_with_party, dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Independent variable\n",
    "multiple_party_votes_all['vote_with_party'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Senate': 717, 'House': 1643})"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure both chambers are covered\n",
    "from collections import Counter\n",
    "chambers = []\n",
    "for i in responses:\n",
    "    chamber = responses[i]['results']['votes']['vote']['chamber']\n",
    "    chambers.append(chamber)\n",
    "d = Counter(chambers)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time of day\n",
    "import pandas as pd\n",
    "multiple_party_votes_all['votes_hour'] = [x.hour for x in pd.to_datetime(multiple_party_votes_all['time'], format = '%H:%M:%S')]\n",
    "bins = [0, 5, 12, 18, 22, 24]\n",
    "labels = ['early_morning', 'morning', 'afternoon', 'evening', 'night']\n",
    "multiple_party_votes_all['votes_time_of_day'] = pd.cut(multiple_party_votes_all['votes_hour'], bins = bins, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_party_votes_all = multiple_party_votes_all.loc[:,~multiple_party_votes_all.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_party_votes_all.drop(columns = ['index', 'votes_hour', 'time', 'district', 'date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(778850, 26)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_party_votes_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('votes_by_party.pkl', 'wb')\n",
    "pickle.dump(multiple_party_votes_all, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party and state of member\n",
    "\n",
    "ProPublica Congress API, https://projects.propublica.org/api-docs/congress-api/, last access: 07/30/2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pkl_file = open('votes_by_party.pkl', 'rb')\n",
    "votes_by_party = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('speeches_features.pkl', 'rb')\n",
    "speeches_cleaned = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684\n"
     ]
    }
   ],
   "source": [
    "unique_member_ids = votes_by_party['member_id'].unique()\n",
    "print(len(unique_member_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678\n"
     ]
    }
   ],
   "source": [
    "unique_speaker_id = speeches_cleaned['speaker_bioguide'].unique()\n",
    "print(len(unique_speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695\n"
     ]
    }
   ],
   "source": [
    "members_info_to_acq = set(unique_member_ids).union(set(unique_speaker_id))\n",
    "print(len(members_info_to_acq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None not found\n"
     ]
    }
   ],
   "source": [
    "# Getting information for \n",
    "import requests\n",
    "headers = {'X-API-Key': '######'}\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "members_df = pd.DataFrame()\n",
    "cols = (['chamber', 'congress', 'missed_votes_pct', 'senate_class', \n",
    "            'seniority','state_rank', 'subcommittees', 'leadership_role', \n",
    "                'state', 'votes_with_party_pct', 'party'])\n",
    "for idx, speaker_id in enumerate(members_info_to_acq):\n",
    "    #print(speaker_id)\n",
    "    if pd.isnull(speaker_id) is False:\n",
    "        try:\n",
    "            speaker = 'https://api.propublica.org/congress/v1/members/{speaker}.json'.format(speaker = speaker_id)\n",
    "            response = requests.get(speaker, headers = headers)\n",
    "            member = json_normalize(response.json(), ['results'])\n",
    "            #print(member.shape)\n",
    "            member = member[['current_party', 'date_of_birth', 'first_name', 'gender', \n",
    "                             'in_office', 'last_name', 'member_id']]\n",
    "            member['full_name'] = member['first_name'] + ' ' + member['last_name']\n",
    "            roles = json_normalize(response.json(), ['results', 'roles'])\n",
    "            #print(roles.shape)\n",
    "            roles_col = [col for col in roles.columns if col in cols]\n",
    "            roles = roles[roles_col]\n",
    "            roles['member_id'] = member['member_id'][0]\n",
    "            member_roles = roles.merge(member, on = 'member_id', how = 'left')\n",
    "            member_roles = member_roles.add_prefix('members_')\n",
    "            if members_df.empty is True:\n",
    "                members_df = member_roles\n",
    "            else:\n",
    "                members_df = pd.concat([members_df, member_roles], axis = 0, join = 'outer')\n",
    "        except(KeyError):\n",
    "            print(speaker_id, \"not found\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {'X-API-Key': '######'}\n",
    "\n",
    "def get_data(url):\n",
    "    responses = []\n",
    "    response = requests.get(url, headers = headers)\n",
    "    #print(response.json())\n",
    "    responses.append(response.json())\n",
    "    try:\n",
    "        num_results = response.json()['num_results']\n",
    "        print(num_results)\n",
    "        for req in range(0,num_results,20):\n",
    "            response = requests.get(url + '{num}'.format(num = req), headers = headers)\n",
    "            responses.append(response.json())\n",
    "        return responses\n",
    "    except(KeyError):\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Getting years to next election for those currently in Congress\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "senate = 'https://api.propublica.org/congress/v1/115/senate/members.json'\n",
    "house = 'https://api.propublica.org/congress/v1/115/house/members.json'\n",
    "\n",
    "members_df_current = pd.DataFrame()\n",
    "for affiliation in [senate, house]:\n",
    "    party = get_data(affiliation)\n",
    "    members = json_normalize(party, ['results','members'])\n",
    "    results = json_normalize(party, 'results')\n",
    "    if members_df_current.empty is True:\n",
    "        members_df_current = members\n",
    "    else:\n",
    "        members_df_current = pd.concat([members_df_current, members], axis = 0, join = 'outer', ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine members_df_current and members_df\n",
    "list_congress = ['113', '114', '115']\n",
    "members_df_recent = members_df[(members_df['members_congress'].isin(list_congress))]\n",
    "members_df_current['members_yrs_until_nxt_election'] = members_df_current['next_election'].map(int) - 2018\n",
    "members_df_all = members_df_recent.merge(members_df_current, left_on = 'members_member_id', right_on = 'id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how old is the member?\n",
    "members_df_all['members_age'] =  [(2018 - x.year) for x in pd.to_datetime(members_df_all['members_date_of_birth'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "members_df_all = (members_df_all[['members_chamber', \n",
    "        'members_full_name',\n",
    "       'members_gender', 'members_in_office', \n",
    "       'members_leadership_role', 'members_member_id',\n",
    "       'members_missed_votes_pct', 'members_party', 'members_senate_class',\n",
    "       'members_seniority', 'members_state', 'members_state_rank',\n",
    "        'members_votes_with_party_pct', \n",
    "       'dw_nominate',  'gender',  'leadership_role', 'missed_votes_pct', \n",
    "       'votes_with_party_pct', 'members_yrs_until_nxt_election',\n",
    "       'members_age', 'members_congress']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('members.pkl', 'wb')\n",
    "pickle.dump(members_df_all, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swing state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {'X-API-Key': '######'}\n",
    "\n",
    "def get_data(url):\n",
    "    responses = []\n",
    "    response = requests.get(url, headers = headers)\n",
    "    #print(response.json())\n",
    "    responses.append(response.json())\n",
    "    try:\n",
    "        num_results = response.json()['num_results']\n",
    "        print(num_results)\n",
    "        for req in range(0,num_results,20):\n",
    "            response = requests.get(url + '{num}'.format(num = req), headers = headers)\n",
    "            responses.append(response.json())\n",
    "        return responses\n",
    "    except(KeyError):\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get party membership counts for all states (current Congress only)\n",
    "from pandas.io.json import json_normalize\n",
    "party_counts_by_state = 'https://api.propublica.org/congress/v1/states/members/party.json'\n",
    "state_party = get_data(party_counts_by_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chambers = pd.DataFrame()\n",
    "for cham in ['house', 'senate']:\n",
    "    keys, values = [], []\n",
    "    for state in state_party[0]['results'][cham]:\n",
    "        for key, value in state.items():\n",
    "            keys.append(key)\n",
    "            values.append(value[0])\n",
    "    chamber = pd.DataFrame(values, index = keys)\n",
    "    chamber = chamber.add_prefix('{cham}_'.format(cham = cham))\n",
    "    if chambers.empty is True:\n",
    "        chambers = chamber\n",
    "    else:\n",
    "        chambers = pd.concat([chambers, chamber], axis = 1, join = 'outer')\n",
    "chambers = chambers.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiveThirtyEight identifies the states of Colorado, Florida, Iowa, Michigan, Minnesota, Ohio, Nevada, New Hampshire, North Carolina, Pennsylvania, Virginia, and Wisconsin as \"traditional\" swing states that have regularly seen close contests over the last few presidential campaigns.\n",
    "\n",
    "https://fivethirtyeight.com/features/the-odds-of-an-electoral-college-popular-vote-split-are-increasing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chambers['swing_state'] = (chambers.index.isin(\n",
    "    ['CO', 'FL', 'IA', 'MI', 'MN', 'OH', 'NV', 'NH', 'NC', 'PA', 'VI', 'WI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red states / blue states\n",
    "import requests\n",
    "election_results = 'https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state'\n",
    "response = requests.get(election_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_states = (['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', \n",
    "                'Colorado', 'Connecticut', 'Delaware',' D.C.', 'Florida', \n",
    "                ' Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', \n",
    "                'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', \n",
    "                'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', \n",
    "                'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
    "                'New Jersey', 'New Mexico', 'New York', 'North Carolina', \n",
    "                'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
    "                'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', \n",
    "                'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', \n",
    "                'West Virginia', 'Wisconsin', 'Wyoming'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red state if voted Rep >= 3 times in the last 4 elections\n",
    "# blue state if voted Dem >= 3 times in the last 4 elections\n",
    "# swing state if voted twice for both\n",
    "from collections import Counter\n",
    "\n",
    "red_states, blue_states, swing_states = [], [], []\n",
    "for state in list_states:\n",
    "    tds = [x.parent.find_next_siblings('td') for x in soup.find_all(\"a\", text = state)]\n",
    "    most_recent_4_elections = [x.text.strip('\\n') for x in tds[0]][-4:]\n",
    "    c = Counter(most_recent_4_elections)\n",
    "    if c['R'] >= 3:\n",
    "        red_states.append(state)\n",
    "    elif c['D'] >= 3:\n",
    "        blue_states.append(state)\n",
    "    else:\n",
    "        swing_states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_abbrev = ({'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', \n",
    "                   'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO',\n",
    "                   'Connecticut': 'CT', 'Delaware': 'DE', ' D.C.': 'DC', 'Florida': 'FL', \n",
    "                   ' Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID', \n",
    "                   'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', \n",
    "                   'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', \n",
    "                   'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA',\n",
    "                   'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "                   'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', \n",
    "                   'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', \n",
    "                   'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n",
    "                   'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', \n",
    "                   'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', \n",
    "                   'South Carolina': 'SC', 'South Dakota': 'SD', \n",
    "                   'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', \n",
    "                   'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', \n",
    "                   'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_states_abbr = []\n",
    "for color in [red_states, blue_states, swing_states]:\n",
    "    states_abbr = []\n",
    "    for state in color:\n",
    "        states_abbr.append(us_state_abbrev[state])\n",
    "    colors_states_abbr.append(states_abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chambers['red_state'] = chambers.index.isin(colors_states_abbr[0])\n",
    "chambers['blue_state'] = chambers.index.isin(colors_states_abbr[1])\n",
    "chambers['swing_state2'] = chambers.index.isin(colors_states_abbr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chambers.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chambers.rename(columns = {'index': 'state'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chambers = chambers[['state', 'swing_state', 'red_state', 'blue_state', 'swing_state2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('states.pkl', 'wb')\n",
    "pickle.dump(chambers, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge members and swing states\n",
    "import pickle\n",
    "\n",
    "pkl_file = open('members.pkl', 'rb')\n",
    "members = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('states.pkl', 'rb')\n",
    "states = pickle.load(pkl_file)\n",
    "\n",
    "members_states = members.merge(states, left_on = 'members_state', right_on = 'state', how = 'left')\n",
    "\n",
    "# Democrat in red state or Republican in blue state\n",
    "members_states['member_diff_party_state'] = 0\n",
    "members_states.loc[(members_states['red_state'] == True) & (members_states['members_party'] == 'D'),'member_diff_party_state'] = 1\n",
    "members_states.loc[(members_states['blue_state'] == True) & (members_states['members_party'] == 'R'),'member_diff_party_state'] = 1\n",
    "\n",
    "members_states.drop(columns = ['state', 'red_state', 'blue_state'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge mentioned bills, votes and topics (=bills_votes_topics)\n",
    "import pickle\n",
    "\n",
    "pkl_file = open('mentioned_bills.pkl', 'rb')\n",
    "mentioned_bills = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file = open('votes_by_party.pkl', 'rb')\n",
    "votes = pickle.load(pkl_file)\n",
    "votes['bill_id'] = [vote.split('-')[0] for vote in votes['bill_id']]\n",
    "\n",
    "bills_votes = mentioned_bills.merge(votes, left_on = 'votes_api_url', right_on = 'vote_api', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bills_votes['congress'] = bills_votes['congress'].map(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.0    320175\n",
       "115.0    245738\n",
       "113.0    212937\n",
       "Name: congress, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bills_votes['congress'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge speeches_metadata and bills_votes\n",
    "pkl_file = open('speeches_metadata_evidence.pkl', 'rb')\n",
    "speeches_metadata_evidence = pickle.load(pkl_file)\n",
    "\n",
    "speeches_all_bill_data = speeches_metadata_evidence.merge(bills_votes, left_on = ['bill_id', 'congress'], right_on = ['bill_slug', 'congress'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('speeches_all_bill_data.pkl', 'wb')\n",
    "pickle.dump(speeches_all_bill_data, output, protocol=4)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "members_states['members_congress'] = members_states['members_congress'].map(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge speeches_metadata and members_states\n",
    "# Voters\n",
    "speeches_bills_members = speeches_all_bill_data.merge(members_states, left_on = ['member_id', 'congress'], right_on = ['members_member_id', 'members_congress'], how = 'left', suffixes = ('_members','_voters'))\n",
    "# Speaker\n",
    "speaker_columns = ['members_chamber', 'members_senate_class',\n",
    "       'members_seniority', 'members_state_rank', 'members_leadership_role',\n",
    "       'members_state', 'members_votes_with_party_pct', 'members_party',\n",
    "       'members_member_id', 'members_gender', 'members_congress']\n",
    "#speeches_bills_members = speeches_bills_members.merge(members_states[speaker_columns], left_on = ['speaker_bioguide'], right_on = ['members_member_id'], how = 'left', suffixes = ('_voters','_speaker'))\n",
    "# Sponsor\n",
    "members_states = members_states[speaker_columns].add_suffix('_sponsor')\n",
    "speeches_bills_members = speeches_bills_members.merge(members_states, left_on = ['sponsor_id', 'congress'], right_on = ['members_member_id_sponsor', 'members_congress_sponsor'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of mentions of same / opposite affiliation think tank\n",
    "party_abbr_dict = {'Republican': 'R', 'Democrat': 'D'}\n",
    "speeches_bills_members['opp_affiliation_tt_mentioned_no_mean'] = 0\n",
    "speeches_bills_members['same_affiliation_tt_mentioned_no_mean'] = 0\n",
    "for key, value in party_abbr_dict.items():\n",
    "    for idx in range(0,speeches_bills_members.shape[0]):\n",
    "        if speeches_bills_members.loc[idx,'members_party'] == value:\n",
    "            print(speeches_bills_members.loc[idx,'members_party'])\n",
    "            print(speeches_bills_members.loc[idx,'mentions_think_tank_{party}_no_mean'.format(party = key)])\n",
    "            speeches_bills_members.at[idx,'same_affiliation_tt_mentioned_no_mean'] = speeches_bills_members.loc[idx,'mentions_think_tank_{party}_no_mean'.format(party = key)]\n",
    "        elif (speeches_bills_members.loc[idx,'mentions_think_tank_{party}_no_mean'.format(party = key)] !=0) and (speeches_bills_members.loc[idx,'members_party'] != value) and (speeches_bills_members.loc[idx,'members_party'] != 'I'):\n",
    "            speeches_bills_members.at[idx,'opp_affiliation_tt_mentioned_no_mean'] = speeches_bills_members.loc[idx,'mentions_think_tank_{party}_no_mean'.format(party = key)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('speeches_bills_members.pkl', 'wb')\n",
    "pickle.dump(speeches_bills_members, output)\n",
    "\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
