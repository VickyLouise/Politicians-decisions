{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading CRS reports and metadata\n",
    "\n",
    "Source: https://www.everycrsreport.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Sample script to download CRS reports from EveryCRSReport.com.\n",
    "#\n",
    "# EveryCRSReport publishes a listing file at\n",
    "# https://www.everycrsreport.com/reports.csv which has the number,\n",
    "# last publication date, relative URL to a report metadata JSON\n",
    "# file, and the SHA1 hash of the metadata file.\n",
    "#\n",
    "# We use that file to download new reports into:\n",
    "#\n",
    "# reports/reports/xxxxxx.json\n",
    "# reports/files/yyyyy.pdf\n",
    "# reports/files/yyyyy.html\n",
    "#\n",
    "# This script was written in Python 3.\n",
    "\n",
    "import hashlib\n",
    "import urllib.request\n",
    "import io\n",
    "import csv\n",
    "import os, os.path\n",
    "import json\n",
    "import time\n",
    "\n",
    "api_base_url = \"https://www.everycrsreport.com/\"\n",
    "\n",
    "def download_file(url, fn, expected_digest):\n",
    "    # Do we have it already?\n",
    "    if os.path.exists(fn):\n",
    "        # Compute the SHA1 hash of the existing file's contents,\n",
    "        # if we are given a hash.\n",
    "        with open(fn, 'rb') as f:\n",
    "            hasher = hashlib.sha1()\n",
    "            hasher.update(f.read())\n",
    "            digest = hasher.hexdigest()\n",
    "\n",
    "        # Is the existing file up to date?\n",
    "        if digest == expected_digest or expected_digest is None:\n",
    "            # No need to download\n",
    "            return\n",
    "\n",
    "    # Download and save the file.\n",
    "    print(fn + \"...\")\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as resp:\n",
    "            data = resp.read()\n",
    "            with open(fn, 'wb') as f:\n",
    "                f.write(data)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(\"\", e)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Ensure output directories exist.\n",
    "os.makedirs(\"reports/reports\", exist_ok=True)\n",
    "os.makedirs(\"reports/files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute an HTTP request to get the CSV listing file.\n",
    "with urllib.request.urlopen(api_base_url + \"reports.csv\") as resp:\n",
    "    # Parse it as a CSV file.\n",
    "    reader = csv.DictReader(io.StringIO(resp.read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch reports.\n",
    "for report in reader:\n",
    "    # Where will we save this report?\n",
    "    metadata_fn = \"reports/\" + report[\"url\"] # i.e. reports/reports/R1234.json\n",
    "\n",
    "    # Download it if we don't have it or it's modified.\n",
    "    download_file(api_base_url + report[\"url\"], metadata_fn, report[\"sha1\"])\n",
    "\n",
    "    # Also download the PDF/HTML files for the report.\n",
    "    #\n",
    "    # While we could get the most recent PDF and HTML file names from the\n",
    "    # CSV file directly (report[\"latestPDF\"], report[\"latestHTML\"]), this\n",
    "    # script demonstrates how to get all past versions of a report also.\n",
    "    #\n",
    "    # Parse the metadata JSON file to figure out what the PDF/HTML file names are.\n",
    "    \n",
    "    with open(metadata_fn) as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "        # Each report may have multiple versions published.\n",
    "        for version in metadata[\"versions\"]:\n",
    "            # Each report version is published in zero or more file formats.\n",
    "            for report_file in version[\"formats\"]:\n",
    "                print(report_file['format'])\n",
    "                if report_file['format'] == \"HTML\":\n",
    "                    # Where will we save this file?\n",
    "                    file_fn = \"reports/\" + report_file[\"filename\"]\n",
    "\n",
    "                    # Download it if we don't have it or it's modified.\n",
    "                    download_file(api_base_url + report_file[\"filename\"], file_fn, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing reports\n",
    "\n",
    "20k is sufficient! The reports are used as a comparison for the type of language used in speeches and 20k is more than sufficient for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def parse_text(text):\n",
    "    f=codecs.open(text, 'rb', encoding = 'utf-8', errors = 'ignore')\n",
    "    document= BeautifulSoup(f.read(), 'lxml').get_text()\n",
    "    document_2 = document.replace(\"\\n\", \" \").replace(\"\\n\", \" \")\n",
    "    #print(document)\n",
    "    return document_2\n",
    "\n",
    "def check_copyright(soup_text):\n",
    "    copyright_list = re.findall('Â©', soup_text)\n",
    "    if copyright_list:\n",
    "        print('Copyright warning!')\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = '/home/ubuntu/Notebooks/reports/files'\n",
    "\n",
    "list_documents = []\n",
    "for filename in os.listdir(path):\n",
    "    document = parse_text('reports/files/' + str(filename))\n",
    "    if check_copyright(document) is False:\n",
    "        list_documents.append(document)\n",
    "    else:\n",
    "        print(filename)\n",
    "    if len(list_documents) % 1000 == 0:\n",
    "        print('Number of documents: ', len(list_documents))\n",
    "    # Stopping at 20k!\n",
    "    if len(list_documents) == 20000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('crs_reports.pkl', 'wb')\n",
    "pickle.dump(list_documents, output)\n",
    "\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
